{"_step":18,"info/prev_portfolio_value":101870.6,"step/step_count":8,"market/halted":false,"model_architecture":"MultiBranchTransformer(\n  (hf_proj): Linear(in_features=20, out_features=64, bias=True)\n  (hf_pos_enc): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (hf_encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0-1): 2 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (linear1): Linear(in_features=64, out_features=128, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=128, out_features=64, bias=True)\n        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (activation): GELU(approximate='none')\n      )\n    )\n    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n  )\n  (mf_proj): Linear(in_features=15, out_features=64, bias=True)\n  (mf_pos_enc): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (mf_encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0-1): 2 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (linear1): Linear(in_features=64, out_features=128, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=128, out_features=64, bias=True)\n        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (activation): GELU(approximate='none')\n      )\n    )\n    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n  )\n  (lf_proj): Linear(in_features=10, out_features=64, bias=True)\n  (lf_pos_enc): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lf_encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0-1): 2 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (linear1): Linear(in_features=64, out_features=128, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=128, out_features=64, bias=True)\n        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (activation): GELU(approximate='none')\n      )\n    )\n    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n  )\n  (static_encoder): Sequential(\n    (0): Linear(in_features=15, out_features=64, bias=True)\n    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n    (2): GELU(approximate='none')\n    (3): Linear(in_features=64, out_features=64, bias=True)\n    (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n    (5): GELU(approximate='none')\n  )\n  (fusion): AttentionFusion(\n    (self_attention): MultiheadAttention(\n      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n    )\n    (proj): Sequential(\n      (0): Linear(in_features=256, out_features=256, bias=True)\n      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (2): GELU(approximate='none')\n      (3): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (actor): ActorNetwork(\n    (shared): Sequential(\n      (0): Linear(in_features=256, out_features=256, bias=True)\n      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (2): GELU(approximate='none')\n      (3): Linear(in_features=256, out_features=256, bias=True)\n      (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (5): GELU(approximate='none')\n    )\n    (mean): Linear(in_features=256, out_features=1, bias=True)\n  )\n  (critic): CriticNetwork(\n    (critic): Sequential(\n      (0): Linear(in_features=256, out_features=256, bias=True)\n      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (2): GELU(approximate='none')\n      (3): Linear(in_features=256, out_features=256, bias=True)\n      (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (5): GELU(approximate='none')\n      (6): Linear(in_features=256, out_features=1, bias=True)\n    )\n  )\n)","update/actor_loss":0.06887382678687573,"episode/trade_count":0,"iteration/update_clip_fraction":0.4875,"info/step_count":1,"info/total_reward":-0.1,"_wandb":{"runtime":2},"step/action":-0.9439438581466675,"iteration/update_approx_kl":0.26283083856105804,"episode/number":8,"step/global_step":15,"market/bid":9.95,"episode/steps":1,"market/price":10,"iteration/update_critic_loss":0.07708676289767027,"iteration/update_entropy":-1.4201561212539673,"info/portfolio_change":1870.6000000000058,"iteration/mean_reward":-0.09999999999999999,"update/entropy":-1.4201561212539673,"episode/win_rate":0,"update/approx_kl":0.26283083856105804,"_runtime":2.3287253,"portfolio/total_value":101870.6,"update/count":0,"update/loss":0.09321564702317119,"iteration/update_loss":0.09321564702317119,"portfolio/cash":100935.3,"episode/length":1,"iteration/update_actor_loss":0.06887382678687573,"market/spread":0.1,"episode/total_pnl_pct":0.01870600000000006,"market/ask":10.05,"update/critic_loss":0.07708676289767027,"iteration/number":1,"_timestamp":1.7472253115948772e+09,"step/reward":-0.1,"update/clip_fraction":0.4875,"episode/total_reward":1870.6000000000058,"episode/reward":-0.1,"iteration/episodes":8,"iteration/steps":8,"episode/total_pnl":1870.6000000000058,"info/portfolio_value":101870.6}