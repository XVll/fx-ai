name: phase3_finetune_optimization
description: "Phase 3: Fine-tuning optimization with fixed foundation and reward parameters from Phases 1-2"
version: "1.0"

dashboard_port: 8052
log_level: INFO
results_dir: sweep_results
save_study_plots: true

studies:
  - study_name: fx_ai_finetune
    direction: maximize
    metric_name: mean_reward
    storage: sqlite:///sweep_studies.db
    load_if_exists: true
    
    # NEW: Base config reference system
    base_config: momentum_training
    
    sampler:
      type: TPESampler      # Fine-grained exploration for remaining parameters
      n_startup_trials: 8
      multivariate: true
      seed: 42
      consider_prior: true
      prior_weight: 1.0
      consider_magic_clip: true
      consider_endpoints: false
    
    pruner:
      type: MedianPruner
      n_startup_trials: 5
      n_warmup_steps: 25
      interval_steps: 5
      n_min_trials: 3
    
    # ONLY fine-tuning parameters (foundation+reward will be set by transfer script)
    parameters:
      - name: training.value_coef
        type: float
        low: 0.2
        high: 1.5
      
      - name: training.gae_lambda
        type: float
        low: 0.85
        high: 0.98
      
      - name: training.clip_epsilon
        type: float
        low: 0.1
        high: 0.3
      
      - name: training.max_grad_norm
        type: float
        low: 0.3
        high: 1.0
      
      - name: training.n_epochs
        type: int
        low: 8
        high: 16
      
      - name: model.n_heads
        type: categorical
        choices: [4, 8, 16]
      
      - name: simulation.commission_rate
        type: float
        low: 0.0005
        high: 0.002
      
      - name: simulation.slippage_rate
        type: float
        low: 0.0001
        high: 0.001
    
    n_trials: 50
    timeout: null
    n_jobs: 1
    catch_exceptions: true
    
    # ONLY trial-specific overrides (foundation+reward params will be set by transfer script)
    trial_overrides:
      mode: train
      experiment_name: sweep_finetune
      
      training:
        checkpoint_interval: 12
        eval_frequency: 12
        eval_episodes: 8
        continue_training: false
        
        # FIXED foundation parameters (populated by transfer script)
        # learning_rate: [SET BY TRANSFER]
        # batch_size: [SET BY TRANSFER]
        # entropy_coef: [SET BY TRANSFER]
        # gamma: [SET BY TRANSFER]
        
        # FIXED reward parameters (populated by transfer script)
        # All reward coefficients from Phase 2 will be transferred
      
      model:
        # FIXED foundation parameters (populated by transfer script)
        # d_model: [SET BY TRANSFER]
        # n_layers: [SET BY TRANSFER]
        # dropout: [SET BY TRANSFER]
        
      env:
        max_episode_steps: 256   # Longer episodes for meaningful trading
        
        reward:
          # FIXED parameters from previous phases (populated by transfer script)
          # pnl_coefficient: [SET BY TRANSFER]
          # holding_penalty_coefficient: [SET BY TRANSFER]
          # drawdown_penalty_coefficient: [SET BY TRANSFER]
          # profit_closing_bonus_coefficient: [SET BY TRANSFER]
          # clean_trade_coefficient: [SET BY TRANSFER]
          # base_multiplier: [SET BY TRANSFER]
          # bankruptcy_penalty_coefficient: [SET BY TRANSFER]
          # profit_giveback_penalty_coefficient: [SET BY TRANSFER]
          # max_drawdown_penalty_coefficient: [SET BY TRANSFER]
          # activity_bonus_per_trade: [SET BY TRANSFER]
          # hold_penalty_per_step: [SET BY TRANSFER]
          # max_holding_time_steps: [SET BY TRANSFER]
      
      wandb:
        enabled: true
        project: fx-ai-sweep-finetune
        tags: [sweep, phase3, finetune]
        notes: "Phase 3: Fine-tuning with fixed foundation and reward parameters"
        save_model: false
      
      dashboard:
        enabled: false
      
      logging:
        file_enabled: false
        log_actions: false
        log_features: false
    
    episodes_per_trial: 4000     # ~40 updates Ã— estimated episodes per update
    eval_frequency: 12
    eval_episodes: 8
    save_checkpoints: false
    checkpoint_dir: sweep_finetune_checkpoints