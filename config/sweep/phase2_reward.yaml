name: phase2_reward_optimization
description: "Phase 2: Reward system optimization with fixed foundation parameters from Phase 1"
version: "1.0"

dashboard_port: 8052
log_level: INFO
results_dir: sweep_results
save_study_plots: true

studies:
  - study_name: fx_ai_reward
    direction: maximize
    metric_name: mean_reward
    storage: sqlite:///sweep_studies.db
    load_if_exists: true
    
    # NEW: Base config reference system
    base_config: momentum_training
    
    sampler:
      type: CmaEsSampler      # Better for continuous reward coefficients
      n_startup_trials: 10
      seed: 42
      warn_independent_sampling: false
    
    pruner:
      type: MedianPruner
      n_startup_trials: 6
      n_warmup_steps: 20
      interval_steps: 4
      n_min_trials: 4
    
    # ONLY reward system parameters (will be populated by transfer script)
    parameters:
      - name: env.reward.pnl_coefficient
        type: float
        low: 80.0
        high: 400.0
      
      - name: env.reward.holding_penalty_coefficient
        type: float
        low: 0.5
        high: 8.0
      
      - name: env.reward.drawdown_penalty_coefficient
        type: float
        low: 2.0
        high: 25.0
      
      - name: env.reward.profit_closing_bonus_coefficient
        type: float
        low: 50.0
        high: 250.0
      
      - name: env.reward.clean_trade_coefficient
        type: float
        low: 10.0
        high: 80.0
      
      - name: env.reward.base_multiplier
        type: int
        low: 2000
        high: 8000
      
      - name: env.reward.bankruptcy_penalty_coefficient
        type: float
        low: 20.0
        high: 100.0
      
      - name: env.reward.profit_giveback_penalty_coefficient
        type: float
        low: 1.0
        high: 8.0
      
      - name: env.reward.max_drawdown_penalty_coefficient
        type: float
        low: 5.0
        high: 30.0
      
      - name: env.reward.activity_bonus_per_trade
        type: float
        low: 0.01
        high: 0.1
      
      - name: env.reward.hold_penalty_per_step
        type: float
        low: 0.005
        high: 0.05
      
      - name: env.reward.max_holding_time_steps
        type: int
        low: 60
        high: 300
    
    n_trials: 75
    timeout: null
    n_jobs: 1
    catch_exceptions: true
    
    # ONLY trial-specific overrides (foundation params will be set by transfer script)
    trial_overrides:
      mode: train
      experiment_name: sweep_reward
      
      training:
        checkpoint_interval: 8
        eval_frequency: 8
        eval_episodes: 5
        continue_training: false
        
        # FIXED foundation parameters (populated by transfer script)
        # learning_rate: [SET BY TRANSFER]
        # batch_size: [SET BY TRANSFER]
        # entropy_coef: [SET BY TRANSFER]
        # gamma: [SET BY TRANSFER]
      
      model:
        # FIXED foundation parameters (populated by transfer script)
        # d_model: [SET BY TRANSFER]
        # n_layers: [SET BY TRANSFER]
        # dropout: [SET BY TRANSFER]
        
      env:
        max_episode_steps: 256   # Longer episodes for meaningful trading
      
      wandb:
        enabled: true
        project: fx-ai-sweep-reward
        tags: [sweep, phase2, reward]
        notes: "Phase 2: Reward system optimization with fixed foundation"
        save_model: false
      
      dashboard:
        enabled: false
      
      logging:
        file_enabled: false
        log_actions: false
        log_features: false
    
    episodes_per_trial: 2500     # ~25 updates Ã— estimated episodes per update
    eval_frequency: 8
    eval_episodes: 5
    save_checkpoints: false
    checkpoint_dir: sweep_reward_checkpoints