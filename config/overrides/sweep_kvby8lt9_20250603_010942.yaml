env:
  reward:
    drawdown_penalty_coefficient: 5.535870878137321
    holding_penalty_coefficient: 4.127677252019599
    pnl_coefficient: 131.11216132415387
model:
  d_model: 256
  dropout: 0.05926137855086379
  n_layers: 6
training:
  batch_size: 32
  clip_epsilon: 0.24373913987835263
  entropy_coef: 0.0269241841819705
  gae_lambda: 0.9789821173068624
  gamma: 0.950652376524898
  learning_rate: 0.00018096817082351575
  n_epochs: 4
