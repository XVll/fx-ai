# @package _global_

# Phase 1: Foundation hyperparameter optimization
# Focuses on core training stability and model architecture

name: phase1_foundation_optimization
description: "Phase 1: Foundation hyperparameter optimization focusing on core training stability and model architecture"
version: "1.0"

dashboard_port: 8052
log_level: INFO
results_dir: sweep_results
generate_plots: true

studies:
  - study_name: fx_ai_foundation
    direction: maximize
    metric_name: mean_reward
    storage: sqlite:///optuna_studies.db
    load_if_exists: true
    
    # Base config to use as starting point
    base_config: config  # Main config file name
    
    # Evaluation settings for optuna trials
    evaluation_interval: 50     # Run evaluation every N updates (matches default)
    evaluation_episodes: 20     # Episodes per evaluation (matches default)
    report_interval: 1          # Report to optuna every evaluation
    use_best_value: false       # Report current value, not best
    
    sampler:
      type: TPESampler
      n_startup_trials: 15
      multivariate: true
      seed: 42
      consider_prior: true
      prior_weight: 1.0
      consider_magic_clip: true
      consider_endpoints: false
      n_ei_candidates: 24
      warn_independent_sampling: true
    
    pruner:
      type: MedianPruner
      n_startup_trials: 8
      n_warmup_steps: 15
      interval_steps: 3
      n_min_trials: 5
    
    # Parameters to optimize
    parameters:
      # Tier 1: Critical training parameters (4 params)
      - name: training.learning_rate
        type: float_log
        low: 0.00005
        high: 0.001
      
      - name: training.batch_size
        type: categorical
        choices: [32, 64, 128]
      
      - name: training.entropy_coef
        type: float_log
        low: 0.001
        high: 0.05
      
      - name: training.gamma
        type: float
        low: 0.97
        high: 0.999
      
      # Tier 2: Core model architecture (3 params)
      - name: model.d_model
        type: categorical
        choices: [64, 128, 256]
      
      - name: model.n_layers
        type: int
        low: 4
        high: 8
      
      - name: model.dropout
        type: float
        low: 0.05
        high: 0.25
      
      # Tier 3: Sample collection efficiency (1 param)
      - name: training.rollout_steps
        type: categorical
        choices: [1024, 2048, 4096]
      
      # Tier 4: Essential reward baseline (1 param)
      - name: env.reward.pnl_coefficient
        type: float
        low: 80.0
        high: 300.0
    
    n_trials: 100
    timeout: null
    n_jobs: 1
    catch_exceptions: true
    
    # Trial-specific config overrides
    trial_overrides:
      experiment_name: optuna_foundation
      
      training:
        # PPO training settings for optuna trials
        n_epochs: 4                      # Standard PPO epochs per update
        batch_size: 64                   # Will be overridden by parameter optimization
        learning_rate: 0.0003            # Will be overridden by parameter optimization
        
        # Training manager settings
        training_manager:
          mode: "training"
          
          # Termination settings for consistent trial length
          termination_max_updates: 200      # Stop after 200 policy updates (4 evaluations)
          termination_max_episodes: null    # No episode limit
          termination_max_cycles: null      # No cycle limit
          intelligent_termination: false    # Disable for consistent trial length
          
          # Data settings - use diverse data for foundation testing
          symbols: ["MLGO"]
          date_range: [null, null]           # Use all available dates
          day_score_range: [0.5, 1.0]       # Medium to high quality days
          reset_roc_range: [0.05, 1.0]      # All reset points
          reset_activity_range: [0.0, 1.0]  # All activity levels
          
          day_selection_mode: "quality"      # Use high quality days
          reset_point_selection_mode: "sequential"
          
          # Rollout settings
          rollout_steps: 1024                # Will be overridden by parameter optimization
          
          # Disable model saving during optimization
          checkpoint_frequency: null
          keep_best_n_models: 1
        
      # Disable heavy callbacks during optimization
      callbacks:
        optuna:
          enabled: true          # Enable optuna callback
        continuous:
          enabled: false         # Disable model saving
        ppo_metrics:
          enabled: true
        execution_metrics:
          enabled: false
        portfolio_metrics:
          enabled: false
        model_metrics:
          enabled: false
        session_metrics:
          enabled: false
        captum_attribution:
          enabled: false
          
      # Disable wandb during optimization
      wandb:
        enabled: false
        
      # Disable dashboard for speed
      dashboard:
        enabled: false