# @package _global_

# Phase 2: Reward system optimization
# Optimizes reward coefficients with fixed foundation parameters

name: phase2_reward_optimization
description: "Phase 2: Reward system optimization with fixed foundation parameters from Phase 1"
version: "1.0"

dashboard_port: 8052
log_level: INFO
results_dir: sweep_results
generate_plots: true

studies:
  - study_name: fx_ai_reward
    direction: maximize
    metric_name: mean_reward
    storage: sqlite:///optuna_studies.db
    load_if_exists: true
    
    # Base config to use as starting point
    base_config: config
    
    # Evaluation settings for optuna trials
    evaluation_interval: 50     # Run evaluation every N updates (matches default)
    evaluation_episodes: 20     # Episodes per evaluation (matches default)
    report_interval: 1          # Report to optuna every evaluation
    use_best_value: false       # Report current value, not best
    
    sampler:
      type: CmaEsSampler      # Better for continuous reward coefficients
      n_startup_trials: 10
      seed: 42
      warn_independent_sampling: false
    
    pruner:
      type: MedianPruner
      n_startup_trials: 6
      n_warmup_steps: 20
      interval_steps: 4
      n_min_trials: 4
    
    # Reward system parameters to optimize
    parameters:
      - name: env.reward.pnl_coefficient
        type: float
        low: 80.0
        high: 400.0
      
      - name: env.reward.holding_penalty_coefficient
        type: float
        low: 0.5
        high: 8.0
      
      - name: env.reward.drawdown_penalty_coefficient
        type: float
        low: 2.0
        high: 25.0
      
      - name: env.reward.profit_closing_bonus_coefficient
        type: float
        low: 50.0
        high: 250.0
      
      - name: env.reward.clean_trade_coefficient
        type: float
        low: 10.0
        high: 80.0
      
      - name: env.reward.base_multiplier
        type: int
        low: 2000
        high: 8000
      
      - name: env.reward.bankruptcy_penalty_coefficient
        type: float
        low: 20.0
        high: 100.0
      
      - name: env.reward.profit_giveback_penalty_coefficient
        type: float
        low: 1.0
        high: 8.0
      
      - name: env.reward.max_drawdown_penalty_coefficient
        type: float
        low: 5.0
        high: 30.0
      
      - name: env.reward.activity_bonus_per_trade
        type: float
        low: 0.01
        high: 0.1
      
      - name: env.reward.hold_penalty_per_step
        type: float
        low: 0.005
        high: 0.05
      
      - name: env.reward.max_holding_time_steps
        type: int
        low: 60
        high: 300
    
    n_trials: 75
    timeout: null
    n_jobs: 1
    catch_exceptions: true
    
    # Trial-specific config overrides
    trial_overrides:
      experiment_name: optuna_reward
      
      training:
        # PPO training settings with fixed foundation parameters
        n_epochs: 4                      # Standard PPO epochs
        
        # TODO: Set these to best values from Phase 1 foundation optimization
        learning_rate: 0.0003            # FIXME: Use best from foundation phase
        batch_size: 64                   # FIXME: Use best from foundation phase  
        entropy_coef: 0.01               # FIXME: Use best from foundation phase
        gamma: 0.99                      # FIXME: Use best from foundation phase
        
        # Training manager settings
        training_manager:
          mode: "training"
          
          # Termination settings - longer for reward tuning
          termination_max_updates: 150      # Stop after 150 policy updates (3 evaluations)
          termination_max_episodes: null    # No episode limit
          termination_max_cycles: null      # No cycle limit
          intelligent_termination: false    # Disable for consistent trial length
          
          # Use same data settings as foundation
          symbols: ["MLGO"]
          date_range: [null, null]
          day_score_range: [0.5, 1.0]       # Medium to high quality days
          reset_roc_range: [0.05, 1.0]
          reset_activity_range: [0.0, 1.0]
          
          day_selection_mode: "quality"
          reset_point_selection_mode: "sequential"
          
          # Fixed rollout steps from foundation
          rollout_steps: 2048              # FIXME: Use best from foundation phase
          
          # Disable model saving during optimization
          checkpoint_frequency: null
          keep_best_n_models: 1
      
      model:
        # TODO: Set these to best values from Phase 1 foundation optimization
        # FIXME: Use best architecture from foundation phase
        d_model: 128                     # FIXME: Use best from foundation phase
        n_layers: 6                      # FIXME: Use best from foundation phase
        dropout: 0.1                     # FIXME: Use best from foundation phase
      
      # Disable heavy callbacks during optimization
      callbacks:
        optuna:
          enabled: true
        continuous:
          enabled: false
        ppo_metrics:
          enabled: true
        execution_metrics:
          enabled: false
        portfolio_metrics:
          enabled: false
        model_metrics:
          enabled: false
        session_metrics:
          enabled: false
        captum_attribution:
          enabled: false
          
      # Disable wandb during optimization
      wandb:
        enabled: false
        
      # Disable dashboard for speed
      dashboard:
        enabled: false