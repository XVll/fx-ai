# @package _global_

# Phase 3: Fine-tuning optimization
# Fine-tunes remaining parameters with fixed foundation and reward parameters

name: phase3_finetune_optimization
description: "Phase 3: Fine-tuning optimization with fixed foundation and reward parameters"
version: "1.0"

dashboard_port: 8052
log_level: INFO
results_dir: sweep_results
generate_plots: true

studies:
  - study_name: fx_ai_finetune
    direction: maximize
    metric_name: mean_reward
    storage: sqlite:///optuna_studies.db
    load_if_exists: true
    
    # Base config to use as starting point
    base_config: config
    
    # Evaluation settings for optuna trials
    evaluation_interval: 50     # Run evaluation every N updates (matches default)
    evaluation_episodes: 20     # Episodes per evaluation (matches default)
    report_interval: 1          # Report to optuna every evaluation
    use_best_value: true        # Report best value for stability in final phase
    
    sampler:
      type: TPESampler
      n_startup_trials: 10
      multivariate: true
      seed: 42
      consider_prior: true
      prior_weight: 1.0
      n_ei_candidates: 24
    
    pruner:
      type: PercentilePruner
      percentile: 75.0        # Keep top 25% of trials
      n_startup_trials: 10
      n_warmup_steps: 30
      interval_steps: 5
      n_min_trials: 5
    
    # Fine-tuning parameters
    parameters:
      # PPO algorithm fine-tuning
      - name: training.value_loss_coef
        type: float
        low: 0.1
        high: 1.0
      
      - name: training.max_grad_norm
        type: float
        low: 0.1
        high: 2.0
      
      - name: training.clip_range
        type: float
        low: 0.1
        high: 0.3
      
      - name: training.clip_range_vf
        type: float
        low: 5.0
        high: 50.0
      
      - name: training.gae_lambda
        type: float
        low: 0.9
        high: 0.99
      
      # Advanced model parameters
      - name: model.n_heads
        type: categorical
        choices: [4, 8, 16]
      
      - name: model.max_position_embeddings
        type: categorical
        choices: [512, 1024, 2048]
      
      # Environment fine-tuning
      - name: env.initial_cash
        type: categorical
        choices: [50000, 75000, 100000]
      
      - name: env.max_position_size
        type: categorical
        choices: [0.2, 0.3, 0.4]
    
    n_trials: 50
    timeout: null
    n_jobs: 1
    catch_exceptions: true
    
    # Trial-specific config overrides
    trial_overrides:
      experiment_name: optuna_finetune
      
      training:
        # PPO training settings with fixed foundation & reward parameters
        n_epochs: 4                      # Standard PPO epochs
        
        # TODO: Set these to best values from Phase 1 & 2
        learning_rate: 0.0003            # FIXME: Use best from foundation phase
        batch_size: 64                   # FIXME: Use best from foundation phase
        entropy_coef: 0.01               # FIXME: Use best from foundation phase
        gamma: 0.99                      # FIXME: Use best from foundation phase
        
        # Fine-tuning parameters (will be optimized)
        value_coef: 0.5                  # Will be overridden by parameter optimization
        max_grad_norm: 0.5               # Will be overridden by parameter optimization
        clip_epsilon: 0.2                # Will be overridden by parameter optimization
        gae_lambda: 0.95                 # Will be overridden by parameter optimization
        
        # Training manager settings
        training_manager:
          mode: "training"
          
          # Termination settings - longer for fine-tuning
          termination_max_updates: 300      # Stop after 300 policy updates (6 evaluations)
          termination_max_episodes: null    # No episode limit
          termination_max_cycles: null      # No cycle limit
          intelligent_termination: false    # Disable for consistent trial length
          
          # Use higher quality data for fine-tuning
          symbols: ["MLGO"]
          date_range: [null, null]
          day_score_range: [0.7, 1.0]       # High quality days only
          reset_roc_range: [0.1, 1.0]       # Better reset points
          reset_activity_range: [0.2, 1.0]  # More active periods
          
          day_selection_mode: "quality"
          reset_point_selection_mode: "quality"  # Use best reset points
          
          # Fixed rollout steps from foundation
          rollout_steps: 2048              # FIXME: Use best from foundation phase
          
          # Disable model saving during optimization
          checkpoint_frequency: null
          keep_best_n_models: 1
      
      model:
        # TODO: Set these to best values from foundation phase
        d_model: 128                     # FIXME: Use best from foundation phase
        n_layers: 6                      # FIXME: Use best from foundation phase
        dropout: 0.1                     # FIXME: Use best from foundation phase
        
        # Fine-tuning parameters (will be optimized)
        n_heads: 8                       # Will be overridden by parameter optimization
        max_position_embeddings: 1024    # Will be overridden by parameter optimization
      
      env:
        # TODO: Set reward parameters to best values from Phase 2
        reward:
          # FIXME: Use best reward coefficients from reward phase
          pnl_coefficient: 150.0         # FIXME: Use best from reward phase
          holding_penalty_coefficient: 2.0  # FIXME: Use best from reward phase
          # ... other reward parameters
          
        # Environment fine-tuning (will be optimized)
        initial_cash: 75000              # Will be overridden by parameter optimization
        max_position_size: 0.3           # Will be overridden by parameter optimization
      
      # Disable heavy callbacks during optimization
      callbacks:
        optuna:
          enabled: true
        continuous:
          enabled: false
        ppo_metrics:
          enabled: true
        execution_metrics:
          enabled: true   # Enable for fine-tuning
        portfolio_metrics:
          enabled: true   # Enable for fine-tuning
        model_metrics:
          enabled: false
        session_metrics:
          enabled: false
        captum_attribution:
          enabled: false
          
      # Disable wandb during optimization
      wandb:
        enabled: false
        
      # Disable dashboard for speed
      dashboard:
        enabled: false